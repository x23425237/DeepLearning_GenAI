{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x23425237/DeepLearning_GenAI/blob/main/textClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN-Based Text Classification Model:**\n",
        "\n",
        "Input: Raw text data (e.g., product descriptions).\n",
        "\n",
        "Step 1: Text is processed and tokenized.\n",
        "\n",
        "Step 2: The tokenized words are converted into dense vectors (via the embedding layer).\n",
        "\n",
        "Step 3: Convolutional filters detect patterns in the sequence of words.\n",
        "\n",
        "Step 4: Max pooling reduces dimensionality, keeping the most important features.\n",
        "\n",
        "Step 5: The output of pooling is passed through dense layers to make final predictions.\n",
        "\n",
        "Step 6: The output layer assigns a probability to each class using softmax."
      ],
      "metadata": {
        "id": "dPB4oRvaTh73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTj56u8pre_q"
      },
      "outputs": [],
      "source": [
        "#!pip install spacy\n",
        "!pip install contractions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXV6KqUjb_Ty"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWGDnr7Qt_9J"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOlXy_3koWDA"
      },
      "outputs": [],
      "source": [
        "# https://www.datacamp.com/tutorial/text-classification-python\n",
        "#https://www.kaggle.com/code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec\n",
        "import opendatasets as op\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSA0lk9ot-Js"
      },
      "outputs": [],
      "source": [
        "# Downloads\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctPe4tfghjnW"
      },
      "outputs": [],
      "source": [
        "\n",
        "op.download(\"https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\")\n",
        "#bharathidohno\n",
        "#66f0d6ee443677c5bc1d92ec501a207b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrQ0GCNkh4tD"
      },
      "outputs": [],
      "source": [
        "filepath=\"/content/ecommerce-text-classification/ecommerceDataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55RVz3oiiP8x"
      },
      "outputs": [],
      "source": [
        "\n",
        "df=pd.read_csv(filepath,names=[\"label\",\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GicbdQUvibrI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-pFGAoTiP0C"
      },
      "outputs": [],
      "source": [
        "df.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRLXa-DbiPsJ"
      },
      "outputs": [],
      "source": [
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7WYX-nmj2Th"
      },
      "outputs": [],
      "source": [
        "df['text'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvPBmQ0rkXLu"
      },
      "outputs": [],
      "source": [
        "# find unique values in label column.\n",
        "#['Household' 'Books' 'Clothing & Accessories' 'Electronics']\n",
        "print(df[\"label\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT9KwSjJdIC6"
      },
      "outputs": [],
      "source": [
        "# Use apply() with a lambda function to create a new column\n",
        "df['new_label'] = df['label'].apply(\n",
        "    lambda x: 0 if x == 'Books' else (1 if x == 'Clothing & Accessories' else (2 if x == 'Electronics' else 3))\n",
        ")\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpPbdzWBk30l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkSs4GPWmPEm"
      },
      "outputs": [],
      "source": [
        "print(df[\"new_label\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsg2FAkHmb3U"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yULZ6rpFm3R8"
      },
      "outputs": [],
      "source": [
        "df_filtered = df[df['new_label'] == 0]\n",
        "print(df_filtered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap35QRWenKhQ"
      },
      "outputs": [],
      "source": [
        "df_filtered[\"text\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4HT05dhnr3m"
      },
      "outputs": [],
      "source": [
        "df_filtered2 = df[df['new_label'] == 2]\n",
        "print(df_filtered2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G2y_mvHnx2t"
      },
      "outputs": [],
      "source": [
        "df_filtered2[\"text\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quiCTpU6K3Nf"
      },
      "outputs": [],
      "source": [
        "# find duplicates\n",
        "isduplicate=df.duplicated()\n",
        "print(isduplicate)\n",
        "\n",
        "# count duplicates\n",
        "print(isduplicate.sum())# 22622 duplicates identified\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwnaBj5ML3C3"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "df=df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKbOKjGymmB5"
      },
      "outputs": [],
      "source": [
        "# locate missing values\n",
        "# print missing values\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ruo4ko4Hm_IS"
      },
      "outputs": [],
      "source": [
        "# replace missing value with non\n",
        "df.fillna(\"non\",inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhNEdCFjn-gc"
      },
      "outputs": [],
      "source": [
        "print(df['new_label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkfsbpOYn3rN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Count of each label\n",
        "label_counts = df['new_label'].value_counts()\n",
        "\n",
        "# Plot\n",
        "label_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title(\"Count of Rows per Label\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning Functions:**\n",
        "* remove_special_char(text)\n",
        "Removes all punctuation and special characters, keeping only letters, numbers, and spaces.\n",
        "\n",
        "* strip_whitespace(text)\n",
        "Strips leading/trailing spaces and replaces multiple spaces with a single space.\n",
        "\n",
        "* remove_numbers(text)\n",
        "Removes all digits from the text."
      ],
      "metadata": {
        "id": "UUeNGnK1DuIO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAeouM9et57V"
      },
      "source": [
        "Remove all special characters\n",
        "Lowercase all the words\n",
        "Tokenize\n",
        "Remove stopwords\n",
        "Lemmatize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword Removal:**\n",
        "\n",
        "* remove_stopwords(text) (using NLTK)\n",
        "Tokenizes the text and removes common English stopwords (like \"the\", \"is\", \"in\") using the NLTK library.\n",
        "\n",
        "* remove_stopwords_spacy(text) (using spaCy)\n",
        "Uses spaCyâ€™s pre-trained language model to identify and remove stopwords more contextually."
      ],
      "metadata": {
        "id": "GmFO8PhjD9em"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVA7UPtgwzRL"
      },
      "outputs": [],
      "source": [
        "# Function to remove punctuation and special characters\n",
        "def remove_special_char(text):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "#Stripping Extra Whitespace\n",
        "def strip_whitespace(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "#Removing Numbers\n",
        "def remove_numbers(text):\n",
        "    cleaned_text = re.sub(r'\\d+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "#stop words - nltk\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def remove_stopwords_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**remove_duplicates**(text)\n",
        "Removes duplicate words from a string while preserving their original order.\n",
        "\n",
        "**normalize_text**(text)\n",
        "A full text normalization pipeline that performs:\n",
        "\n",
        "* Unicode normalization\n",
        "Removes non-ASCII characters for clean encoding.\n",
        "\n",
        "* Contraction expansion\n",
        "Converts \"don't\" to \"do not\", \"it's\" to \"it is\", using the contractions library.\n",
        "\n",
        "* Lowercasing\n",
        "Standardizes text to lowercase for uniformity.\n",
        "\n",
        "* Punctuation removal\n",
        "Strips all punctuation using regex.\n",
        "\n",
        "* Whitespace cleanup\n",
        "Removes extra or irregular spaces.\n",
        "\n",
        "* NLP parsing with spaCy:\n",
        "\n",
        "* Filters out stopwords and non-alphabetic tokens.\n",
        "\n",
        "* Lemmatizes tokens\n",
        "\n",
        "* Deduplication\n",
        "Final step removes repeated words using remove_duplicates().\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSfSiYEHFVFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySxWzj4DorJA"
      },
      "outputs": [],
      "source": [
        "# remove duplicates\n",
        "def remove_duplicates(text):\n",
        "    seen = set()\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "        if word not in seen:\n",
        "            seen.add(word)\n",
        "            result.append(word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Unicode normalization\n",
        "    text = text.encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Spelling correction (basic)\n",
        "    #text = str(TextBlob(text).correct())\n",
        "\n",
        "    # Tokenization and NLP parsing\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words \\\n",
        "           and token.is_alpha \\\n",
        "           and token.pos_ in {'NOUN', 'ADJ', 'VERB'}:\n",
        "            # Lemmatization\n",
        "            tokens.append(token.lemma_)\n",
        "\n",
        "    return remove_duplicates(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmO1M3U7pcIc"
      },
      "outputs": [],
      "source": [
        "# apply normalise text functions to sample text\n",
        "\n",
        "sample1=df_filtered2[\"text\"].iloc[9]\n",
        "\n",
        "print(sample1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yHpHgfPpucL"
      },
      "outputs": [],
      "source": [
        "normalised=normalize_text(sample1)\n",
        "print(sample1)\n",
        "print(normalised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xLyddvIp8SC"
      },
      "outputs": [],
      "source": [
        "df[\"normalised_text\"]=df[\"text\"].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FiHuxiGqkOv"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKEpNQHR1mgw"
      },
      "outputs": [],
      "source": [
        "# count number of words in each row in normalised_text column . create new column\n",
        "df['word_count'] = df['normalised_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY8pyqgl11QM"
      },
      "outputs": [],
      "source": [
        "# max word_count by lable\n",
        "average_word_count = df.groupby('label')['word_count'].max()\n",
        "\n",
        "average_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXPnwbo53VMn"
      },
      "outputs": [],
      "source": [
        "# select word_count where label=books\n",
        "df_books = df[df['label'] == 'Books']['word_count']\n",
        "df_clothing = df[df['label'] == 'Clothing & Accessories']['word_count']\n",
        "df_Electronics = df[df['label'] == 'Electronics']['word_count']\n",
        "df_Household = df[df['label'] == 'Household']['word_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HozzblKS6Gs1"
      },
      "outputs": [],
      "source": [
        "# scatter plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "axs[0,0].scatter(df_books, df_books, color='skyblue', edgecolor='black')\n",
        "axs[0,1].scatter(df_clothing, df_clothing, color='yellow', edgecolor='black')\n",
        "axs[1,0].scatter(df_Electronics, df_Electronics, color='green', edgecolor='black')\n",
        "axs[1,1].scatter(df_Household, df_Household, color='pink', edgecolor='black')\n",
        "\n",
        "axs[0,0].set_title('Books')\n",
        "axs[0,1].set_title('Clothing & Accessories')\n",
        "axs[1,0].set_title('Electronics')\n",
        "axs[1,1].set_title('Household')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOkPyu0W6sX7"
      },
      "outputs": [],
      "source": [
        "# violin plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "axs[0,0].violinplot(df_books, showmeans=True)\n",
        "axs[0,1].violinplot(df_clothing, showmeans=True)\n",
        "axs[1,0].violinplot(df_Electronics, showmeans=True)\n",
        "axs[1,1].violinplot(df_Household, showmeans=True)\n",
        "\n",
        "axs[0,0].set_title('Books')\n",
        "axs[0,1].set_title('Clothing & Accessories')\n",
        "axs[1,0].set_title('Electronics')\n",
        "axs[1,1].set_title('Household')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshIBgo28rVZ"
      },
      "outputs": [],
      "source": [
        "# select only normalised_text and new_lable fields\n",
        "df_new=df[[\"normalised_text\",\"new_label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGWaaZPanv9g"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyKzhhTo9tId"
      },
      "outputs": [],
      "source": [
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu_Lmplx251Z"
      },
      "outputs": [],
      "source": [
        "df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** and Padding:\n",
        "* Tokenizer: Converts words into integers based on frequency (max_words=1000).\n",
        "\n",
        "* Sequences: Converts text to sequences of tokens.\n",
        "\n",
        "* Padding: Ensures all sequences have the same length (max_len=100) for model input."
      ],
      "metadata": {
        "id": "7Edr4JM1GHXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DJ7wvTrKJW8"
      },
      "outputs": [],
      "source": [
        "# APPLimport pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "\n",
        "\n",
        "text=df_new[\"normalised_text\"]\n",
        "labels=df_new[\"new_label\"]\n",
        "\n",
        "max_words=1000\n",
        "max_len=100\n",
        "tokenizer=Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "\n",
        "sequences=tokenizer.texts_to_sequences(text)\n",
        "X=pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "y = to_categorical(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional** Neural Network (CNN) for Text Classification\n",
        "Purpose: The model is designed to classify text into different categories (multi-class classification), such as categorizing e-commerce product reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "6rVPzVMOGhlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l85TeG4KJOe"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y.shape[1], activation='softmax')  # y.shape[1] gives number of classes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Quq2qbKJBT"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "qSl6XA5hH0MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=model.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          batch_size=64,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "GD7I2OnFIYhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract training and validation accuracy and convert to percentage\n",
        "train_acc = output.history['accuracy'] * 100\n",
        "val_acc = output.history['val_accuracy'] * 100\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc}\")\n",
        "print(f\"Validation Accuracy: {val_acc}\")"
      ],
      "metadata": {
        "id": "TjrciFE2LlhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "McDlrQ2lIajK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate the confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xhxtJoArHG8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kv48zMXWHymf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impliment BERT\n",
        "\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "candidate_labels = ['Household', 'Books' ,'Clothing & Accessories', 'Electronics']\n",
        "\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "4wsouEKTKP0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[19000]"
      ],
      "metadata": {
        "id": "gc8viDFHPi54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.iloc[19000]"
      ],
      "metadata": {
        "id": "TdYzUHBhOrhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample2=df_new[\"normalised_text\"].iloc[19000]\n",
        "print(sample2)"
      ],
      "metadata": {
        "id": "AiYzn0adNc8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_classify=sample2\n",
        "\n",
        "classifier(text_classify,candidate_labels)   # 97% for work\n",
        "\n"
      ],
      "metadata": {
        "id": "PFn-gmp1KdmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ref:\n",
        "# https://www.geeksforgeeks.org/text-classification-using-cnn/\n",
        "# https://cnvrg.io/cnn-sentence-classification/\n",
        "# https://www.mdpi.com/1999-5903/12/12/228\n",
        "\n"
      ],
      "metadata": {
        "id": "ijPS6014KmtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6q2s9i6K6gw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}